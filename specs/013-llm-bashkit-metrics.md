# Spec 013: LLM-Bashkit Metrics

## Purpose

Define the metrics that measure how effectively LLMs use Bashkit as a bash tool. These metrics serve three audiences: library consumers evaluating Bashkit for their agent stack, model developers benchmarking bash tool-use quality, and Bashkit maintainers identifying interpreter gaps.

## Metric Categories

### 1. Task Completion

**What it measures:** Whether the LLM achieves the intended goal through Bashkit.

| Metric | Definition | Granularity |
|--------|-----------|-------------|
| **Tasks passed** | Count of tasks where every expectation check passes | Per-run |
| **Pass rate** | `tasks_passed / total_tasks` | Per-run |
| **Score** | Weighted sum of passed checks / total weight across all tasks | Per-run |
| **Task score** | Weighted sum of passed checks / total weight for one task | Per-task |
| **Category pass rate** | Score aggregated per task category | Per-category |

**Why it matters:** The primary signal of whether an LLM can accomplish real work through Bashkit. A model that passes 95% of tasks is production-viable; one at 80% needs guardrails.

**Scoring details:** Each task defines expectations with optional weights (default 1.0). A task "passes" only if all expectations pass. The overall score is a softer metric — a task that achieves 4/5 checks contributes 80% of its weight even though it counts as failed.

### 2. Tool Call Efficiency

**What it measures:** How the LLM uses the bash tool across conversation turns.

| Metric | Definition | Granularity |
|--------|-----------|-------------|
| **Tool calls** | Total bash invocations across all tasks | Per-run |
| **Tool calls (ok)** | Tool calls with exit_code 0 | Per-run |
| **Tool calls (error)** | Tool calls with exit_code != 0 | Per-run |
| **Tool call success rate** | `tool_calls_ok / total_tool_calls` | Per-run |
| **Turns** | LLM round-trips (each provider.chat() call = 1 turn) | Per-run, per-task |
| **Avg tool calls per task** | `total_tool_calls / total_tasks` | Per-run |
| **Avg turns per task** | `total_turns / total_tasks` | Per-run |

**Why it matters:** Tool call success rate is the sharpest signal for Bashkit compatibility. When a model issues a valid bash command and gets a non-zero exit code, either:
- The model wrote buggy bash (model problem)
- Bashkit doesn't support the syntax or builtin (interpreter gap)

A low success rate (< 75%) indicates significant friction. Comparing success rates across models at similar pass rates isolates interpreter gaps from model skill — if all models fail on the same commands, it's a Bashkit limitation.

**Turns vs. tool calls:** A single turn may contain multiple tool calls (batch execution). Turns measure LLM reasoning steps; tool calls measure interpreter utilization. A model that solves a task in 2 turns with 2 tool calls is more efficient than one using 8 turns with 8 tool calls for the same result.

### 3. Token Economics

**What it measures:** Cost of achieving results through Bashkit.

| Metric | Definition | Granularity |
|--------|-----------|-------------|
| **Input tokens** | Total tokens sent to the LLM (system prompt + conversation history) | Per-run, per-task |
| **Output tokens** | Total tokens generated by the LLM | Per-run, per-task |

**Why it matters:** Token usage drives cost. Bashkit's system prompt and tool description contribute to input tokens on every turn. A model that completes tasks in fewer turns with shorter tool outputs costs less per task.

**What to watch:** Input tokens grow with conversation length (accumulated history). Models that fail and retry accumulate context rapidly — a task hitting the 10-turn limit can consume 5-10x the tokens of a clean 2-turn completion.

### 4. Latency

**What it measures:** Wall-clock time for task completion.

| Metric | Definition | Granularity |
|--------|-----------|-------------|
| **Duration** | Total wall-clock time for all tasks | Per-run |
| **Avg duration per task** | `total_duration_ms / total_tasks` | Per-run |
| **Task duration** | Wall-clock time for a single task (includes LLM latency + interpreter execution) | Per-task |

**Why it matters:** In agentic workflows, bash tool calls are on the critical path. Duration is dominated by LLM inference time, not interpreter execution (Bashkit is 100-1000x faster than subprocess bash for simple operations). Slow models that retry frequently compound both latency and cost.

### 5. Behavioral Signals (per-task trace)

**What it measures:** Qualitative patterns in how models interact with Bashkit.

| Signal | Source | Interpretation |
|--------|--------|---------------|
| **Natural stop** | `trace.natural_stop` | LLM stopped on its own (vs. hitting turn limit). False = model got stuck in a retry loop. |
| **Error-then-adapt** | Sequence of error→different command | Model adjusted to a Bashkit limitation. Positive signal. |
| **Error-then-repeat** | Sequence of error→same command | Model retried the same failing approach. Negative signal. |
| **Commands issued** | `trace.tool_calls[].commands` | Raw bash commands the LLM chose. Reveals which constructs models reach for. |
| **Stderr patterns** | `trace.tool_calls[].stderr` | Common error messages identify missing builtins, unsupported syntax, or ambiguous behavior. |

**Why it matters:** Summary metrics hide important nuance. A model may pass a task but waste 7 turns retrying `sed -i` (not supported) before switching to a redirect approach. The trace reveals whether success came from skill or brute force.

## Expectation Checks

Expectations are the atomic unit of scoring. Each check type validates one aspect of the agent's work.

| Check | Format | Validates |
|-------|--------|-----------|
| `exit_code` | `exit_code:N` | Last tool call returned expected exit code |
| `stdout_contains` | `stdout_contains:text` | Any tool output contains text |
| `stdout_regex` | `stdout_regex:pattern` | Any tool output matches regex |
| `stderr_empty` | `stderr_empty` | No tool call produced stderr |
| `file_exists` | `file_exists:/path` | VFS path exists after task completes |
| `dir_exists` | `dir_exists:/path` | VFS directory exists after task completes |
| `file_contains` | `file_contains:/path:text` | VFS file contains expected text |
| `tool_calls_min` | `tool_calls_min:N` | Model used at least N tool calls |
| `tool_calls_max` | `tool_calls_max:N` | Model used at most N tool calls |

Checks operate against two data sources:
- **Agent trace** — stdout, stderr, exit codes from tool calls (runtime behavior)
- **VFS state** — filesystem after all tool calls complete (persistent artifacts)

This dual validation catches models that produce correct stdout but fail to write results to disk, or vice versa.

## Task Categories

Categories group tasks by bash skill domain. Category-level metrics reveal model strengths and Bashkit coverage gaps.

| Category | Tasks | What it tests |
|----------|-------|--------------|
| file_operations | 3 | mkdir, cp, mv, find, rm — basic VFS interaction |
| text_processing | 4 | grep, sed, awk on structured data — text tool proficiency |
| pipelines | 3 | Multi-stage pipes, command substitution — composition skill |
| scripting | 4 | Variables, arrays, loops, functions — bash programming |
| data_transformation | 5 | CSV↔JSON, log parsing, column reordering — real-world ETL |
| error_recovery | 2 | Missing files, broken JSON — graceful failure handling |
| system_info | 2 | whoami, date, env — sandbox introspection |
| archive_operations | 2 | tar, gzip workflows — binary-adjacent operations |
| json_processing | 8 | jq queries, transforms, merges — structured data manipulation |
| complex_tasks | 4 | Multi-step scenarios combining several categories |

## Cross-Model Comparison

Comparing the same metrics across models on the same dataset reveals:

| Comparison | What it shows |
|------------|--------------|
| Pass rate delta | Model skill gap for bash tool-use |
| Tool call success delta | Whether failure is model-side or interpreter-side |
| Category variance | Which bash domains differentiate models |
| Token ratio at same pass rate | Cost efficiency per unit of capability |
| Turn count at same pass rate | Reasoning efficiency — fewer turns = better planning |

**Key diagnostic:** If all models fail on the same task with similar errors, the issue is likely a Bashkit limitation. If one model passes and others fail, it's a model capability difference.

## Benchmark Metrics (Performance)

Separate from eval (which measures LLM+Bashkit end-to-end), `bashkit-bench` measures raw interpreter performance.

| Metric | Definition |
|--------|-----------|
| **Mean execution time** | Average time to execute a bash snippet (nanoseconds) |
| **Stddev** | Execution time variance |
| **Min / Max** | Execution time bounds |
| **Output match** | Whether Bashkit output matches native bash reference |
| **Error rate** | Fraction of benchmark cases that error |

75 benchmark cases across 9 categories (startup, variables, arithmetic, control, strings, arrays, pipes, tools, complex) compare Bashkit against native bash and just-bash.

**Relevance to LLM workflows:** Bashkit's in-process execution eliminates fork/exec overhead (100-1000x faster for simple commands). This matters for agentic loops where tool calls happen in tight sequences. Performance benchmarks verify this advantage holds across the command surface area that LLMs actually use.

## Interpreting Results

### Healthy Profile
- Pass rate > 90%
- Tool call success rate > 80%
- Avg turns per task < 5
- No category below 70%

### Friction Signals
- Tool call success rate < 75% → Bashkit compatibility gaps
- Single category below 50% → Missing builtins or syntax for that domain
- Avg turns per task > 7 → Models retrying due to errors
- High token usage with low pass rate → Models stuck in retry loops

### Improvement Levers
- **Raise tool call success rate** → Fix interpreter gaps (benefits all models)
- **Add expectations to underspecified tasks** → Better signal from eval
- **Add tasks for uncovered builtins** → Broader coverage measurement
- **Compare across Bashkit versions** → Track regression/improvement

## See Also

- [009-tool-contract.md](009-tool-contract.md) — Tool trait that LLMs interact with
- [012-eval.md](012-eval.md) — Eval harness architecture and dataset format
- [009-implementation-status.md](009-implementation-status.md) — Feature coverage
